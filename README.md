# ğŸ“ˆ Tech Challenge - Fase 4: PrevisÃ£o de AÃ§Ãµes com MLOps

![Python](https://img.shields.io/badge/Python-3.11-blue?style=for-the-badge&logo=python)
![PyTorch](https://img.shields.io/badge/PyTorch-2.0-ee4c2c?style=for-the-badge&logo=pytorch)
![FastAPI](https://img.shields.io/badge/FastAPI-0.109-009688?style=for-the-badge&logo=fastapi)
![Pytest](https://img.shields.io/badge/Pytest-Testing-yellow?style=for-the-badge&logo=pytest)
![Docker](https://img.shields.io/badge/Docker-Container-2496ed?style=for-the-badge&logo=docker)

> **PÃ³s-GraduaÃ§Ã£o em Deep Learning & AI - FIAP**

Este projeto apresenta uma soluÃ§Ã£o completa de **End-to-End Machine Learning** para a previsÃ£o de preÃ§os de fechamento de aÃ§Ãµes da **CEMIG (CMIG4.SA)**.

A arquitetura abrange desde a engenharia de dados atÃ© o deploy produtivo, utilizando **LSTM (Long Short-Term Memory)** para modelagem temporal, **MLflow** para rastreamento de experimentos e **FastAPI** para servir o modelo, tudo orquestrado via **Docker** e validado com testes de integraÃ§Ã£o.

---

## ğŸš€ Funcionalidades e Diferenciais

*   **Pipeline Automatizado:** Scripts modulares para ETL (ExtraÃ§Ã£o e TransformaÃ§Ã£o), Treinamento e AvaliaÃ§Ã£o.
*   **Deep Learning Moderno:** Uso de **PyTorch Lightning** para estruturar o cÃ³digo de treino, garantindo legibilidade e reprodutibilidade (seeds fixas).
*   **Qualidade de Software:** SuÃ­te de **testes de integraÃ§Ã£o** (`pytest`) que valida a API, o carregamento de artefatos e a lÃ³gica de detecÃ§Ã£o de anomalias antes do deploy.
*   **MLOps & Tracking:** IntegraÃ§Ã£o nativa com **MLflow** para registrar mÃ©tricas (MAE, RMSE, MAPE), hiperparÃ¢metros e artefatos do modelo.
*   **API Inteligente & Usabilidade:**
    *   **DetecÃ§Ã£o de Data Drift:** O endpoint de prediÃ§Ã£o monitora estatisticamente a entrada. Se os dados desviarem do padrÃ£o de treino (ex: alta volatilidade), um alerta Ã© retornado.
    *   **Treino AssÃ­ncrono:** O endpoint `/train` utiliza `BackgroundTasks`, permitindo que o modelo seja retreinado sem bloquear as inferÃªncias.
    *   **DocumentaÃ§Ã£o Interativa:** O Swagger UI vem prÃ©-configurado com exemplos de dados e endpoints auxiliares para facilitar o teste manual.
*   **ContainerizaÃ§Ã£o Segura:** Dockerfile otimizado (multi-stage concepts), rodando com usuÃ¡rio nÃ£o-root para seguranÃ§a.

---

## ğŸ—ï¸ Arquitetura e DecisÃµes TÃ©cnicas (ADR)

| Componente | Escolha TÃ©cnica | Justificativa (Why?) |
| :--- | :--- | :--- |
| **Framework DL** | **PyTorch + Lightning** | O PyTorch oferece flexibilidade dinÃ¢mica. O Lightning foi adotado para remover *boilerplate* (loops manuais), padronizar o cÃ³digo e facilitar o uso de *callbacks* (Early Stopping). |
| **Tracking** | **MLflow** | Ferramenta open-source padrÃ£o de mercado, agnÃ³stica de infraestrutura, permitindo rastreabilidade total dos experimentos. |
| **API** | **FastAPI** | Alta performance (ASGI), validaÃ§Ã£o automÃ¡tica de dados com Pydantic e suporte nativo a processamento assÃ­ncrono. |
| **Testes** | **Pytest + TestClient** | PadrÃ£o da indÃºstria para testes em Python. O TestClient permite simular requisiÃ§Ãµes Ã  API sem necessidade de subir o servidor, validando o ciclo de vida (`lifespan`) da aplicaÃ§Ã£o. |
| **Drift Detection** | **EstatÃ­stica (In-App)** | ImplementaÃ§Ã£o de um detector leve baseado em estatÃ­sticas descritivas (Baseline JSON). Evita a complexidade de ferramentas externas pesadas para este escopo, garantindo monitoramento em tempo real. |
| **ConfiguraÃ§Ã£o** | **Single Source of Truth** | Uso de um arquivo `src/config.py` centralizado para evitar "nÃºmeros mÃ¡gicos" e inconsistÃªncias de caminhos entre scripts e API. |

---

## ğŸ“‚ Estrutura do Projeto

```text
/
â”œâ”€â”€ api/                  # AplicaÃ§Ã£o Web (FastAPI)
â”‚   â”œâ”€â”€ main.py           # Endpoints e LÃ³gica de NegÃ³cio
â”‚   â””â”€â”€ __init__.py       # ConfiguraÃ§Ã£o de Logs
â”œâ”€â”€ data/                 # Data Lake Local
â”‚   â”œâ”€â”€ 01_raw/           # Dados brutos (CSV)
â”‚   â””â”€â”€ 02_processed/     # Dados normalizados (.npy)
â”œâ”€â”€ mlruns/               # Registro de Experimentos MLflow
â”œâ”€â”€ models/               # Artefatos Persistidos
â”‚   â”œâ”€â”€ lstm_model.pth    # Pesos do Modelo (State Dict)
â”‚   â”œâ”€â”€ scaler.joblib     # Normalizador (MinMaxScaler)
â”‚   â”œâ”€â”€ baseline_stats.json # EstatÃ­sticas para Drift Detection
â”‚   â””â”€â”€ metrics.json      # MÃ©tricas do Ãºltimo treino (para API)
â”œâ”€â”€ results/              # GrÃ¡ficos de Performance
â”œâ”€â”€ scripts/              # Pipeline de ExecuÃ§Ã£o
â”‚   â”œâ”€â”€ 01_coleta_dados.py
â”‚   â”œâ”€â”€ 02_preprocess.py
â”‚   â”œâ”€â”€ 03_train.py
â”‚   â””â”€â”€ 04_evaluate.py
â”œâ”€â”€ src/                  # CÃ³digo Fonte Compartilhado
â”‚   â”œâ”€â”€ config.py         # ConfiguraÃ§Ãµes Globais
â”‚   â”œâ”€â”€ dataset.py        # Classe Dataset (PyTorch)
â”‚   â””â”€â”€ model.py          # Arquitetura LSTM
â”œâ”€â”€ tests/                # Testes Automatizados
â”‚   â””â”€â”€ test_integration.py
â”œâ”€â”€ Dockerfile            # DefiniÃ§Ã£o da Imagem
â”œâ”€â”€ pyproject.toml        # Gerenciamento de DependÃªncias (Poetry)
â””â”€â”€ README.md             # DocumentaÃ§Ã£o
```

---

## âš¡ Como Executar

### PrÃ©-requisitos
*   Docker (para execuÃ§Ã£o isolada)
*   Python 3.11+ e Poetry (para desenvolvimento local)

### OpÃ§Ã£o 1: Via Docker (Recomendado para ProduÃ§Ã£o)

Esta opÃ§Ã£o sobe a API pronta para uso, contendo o modelo prÃ©-treinado.

1.  **Gerar requirements (caso tenha alterado dependÃªncias):**
    ```bash
    poetry export -f requirements.txt --output requirements.txt --without-hashes
    ```

2.  **Construir a Imagem:**
    ```bash
    docker build -t lstm-mlops .
    ```

3.  **Rodar o Container:**
    ```bash
    docker run -d -p 8000:8000 --name api-lstm lstm-mlops
    ```

4.  **Acessar:**
    *   DocumentaÃ§Ã£o Interativa (Swagger): [http://localhost:8000/docs](http://localhost:8000/docs)

---

### OpÃ§Ã£o 2: ExecuÃ§Ã£o Local (Desenvolvimento)

Siga esta ordem para reproduzir todo o ciclo de vida do modelo.

1.  **InstalaÃ§Ã£o:**
    ```bash
    poetry install
    poetry shell
    ```

2.  **Pipeline de Dados e Treino:**
    ```bash
    # 1. Coleta (Yahoo Finance)
    python -m scripts.01_coleta_dados

    # 2. PrÃ©-processamento (Gera dados .npy e baseline_stats.json)
    # IMPORTANTE: Essencial para o funcionamento do Drift Detection
    python -m scripts.02_preprocess

    # 3. Treinamento (Gera lstm_model.pth e registra no MLflow)
    python -m scripts.03_train

    # 4. AvaliaÃ§Ã£o (Gera mÃ©tricas e grÃ¡ficos em /results)
    python -m scripts.04_evaluate
    ```

3.  **ValidaÃ§Ã£o (Testes Automatizados):**
    Execute a suÃ­te de testes para garantir que a API e o modelo estÃ£o integrados corretamente.
    ```bash
    pytest -v
    ```

4.  **Visualizar Experimentos:**
    ```bash
    mlflow ui
    # Acesse http://127.0.0.1:5000
    ```

5.  **Iniciar API:**
    ```bash
    uvicorn api.main:app --host 0.0.0.0 --port 8000 --reload
    ```

---

## ğŸ”Œ DocumentaÃ§Ã£o da API

A API expÃµe endpoints estratÃ©gicos documentados via Swagger UI.

### 1. `POST /predict` (InferÃªncia)
Recebe uma janela histÃ³rica e prevÃª o prÃ³ximo dia.
*   **Facilidade:** O Swagger jÃ¡ vem preenchido com um exemplo vÃ¡lido.
*   **Input:** Lista com **60 preÃ§os** (float).
*   **Output:** PreÃ§o previsto + Alerta de Drift.

### 2. `GET /sample-data` (Auxiliar)
Retorna os Ãºltimos 60 dias **reais** do dataset de teste.
*   **Uso:** Copie o retorno deste endpoint e cole no `/predict` para validar o modelo com dados reais.

### 3. `POST /train` (Treino & Tuning)
Dispara um job de treinamento em **background**.
*   **Tuning:** Permite enviar novos hiperparÃ¢metros (ex: `learning_rate`, `hidden_size`) no corpo da requisiÃ§Ã£o para ajustar o modelo.

### 4. `GET /model/info` (Monitoramento)
Exibe o estado atual do modelo em produÃ§Ã£o.
*   **Retorno:** VersÃ£o, hiperparÃ¢metros ativos e **mÃ©tricas de performance** (MAE, RMSE) do Ãºltimo treino realizado.

### 5. `GET /health`
Monitoramento de saÃºde (Liveness Probe) e uso de recursos (CPU/RAM).

---

## ğŸ“ˆ Resultados Obtidos

O modelo atual (LSTM 2-Layers, Hidden=64) apresentou nos dados de teste:

| MÃ©trica | Valor | DescriÃ§Ã£o |
| :--- | :--- | :--- |
| **MAPE** | **1.94%** | Erro percentual mÃ©dio absoluto. |
| **RMSE** | **0.25** | Raiz do erro quadrÃ¡tico mÃ©dio (na escala real em R$). |
| **MAE**  | **0.20** | Erro absoluto mÃ©dio (na escala real em R$). |

---

## â˜ï¸ Proposta de Escalabilidade

Para um cenÃ¡rio de alta demanda, a arquitetura evoluiria para:

1.  **Kubernetes (K8s):** OrquestraÃ§Ã£o dos containers.
2.  **HPA (Horizontal Pod Autoscaler):** Escalonamento automÃ¡tico de Pods da API baseado em CPU (>70%) ou mÃ©tricas customizadas de latÃªncia.
3.  **SeparaÃ§Ã£o de Workloads:**
    *   O endpoint `/train` deixaria de processar localmente e enviaria mensagens para uma fila (**RabbitMQ**).
    *   **Workers dedicados (Celery)** consumiriam a fila para treinar modelos em GPUs isoladas, sem impactar a latÃªncia da API de inferÃªncia.

---

## ğŸ‘¥ Autor

**Fernando Luiz Ferreira**
```